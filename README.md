# ğŸ“ Data Versioning and CI/CD with DVC

This project demonstrates a minimal setup for **data versioning using DVC** and **CI/CD using GitHub Actions**, focused on the real-world scenario of managing datasets stored in the cloud (e.g., AWS S3 or GCP Cloud Storage).

---

## ğŸ§© Scenario

This project addresses a common real-world need in data engineering and MLOps:

- âœ… **Version and track datasets** used in data pipelines using DVC.
- â˜ï¸ **Simulate remote data access** from cloud storage providers like AWS S3 or GCP Cloud Storage.
- ğŸ›¡ï¸ **Validate dataset accessibility and structure** before running pipelines â€” helping to:
  - Avoid unnecessary data pushes or downloads
  - Prevent pipeline failures due to missing or malformed files
  - Reduce cloud compute and storage costs

---

## âœ… Current Logic

The script `test_pipeline.py` performs a dry run:

- Attempts to access a CSV file (e.g., `data/sample.csv`).
- Prints the number of rows and columns to verify structure.
- This script is run automatically in CI/CD.

---

## ğŸ”§ To-Do

1. âœ… **Check if file exists** before trying to read it.
2. ğŸŸ¨ Use **partial read** (e.g., `nrows=10`) or **wildcard/glob** to test schema/layout.
3. ğŸŸ¨ **Learn and Setup DVC** (e.g., S3, SSH, or mapped drive).
4. ğŸŸ¨ Optionally add **data schema validation** (e.g., using `pandera` or `great_expectations`).

---

## ğŸ“š Libraries Used

- [**DVC**](https://dvc.org/) â€“ For data versioning, remote data storage, and pipeline tracking
- [**pandas**](https://pandas.pydata.org/) â€“ For reading and validating structured data (CSV, etc.)

---

